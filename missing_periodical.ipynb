{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is scraping articles with ''CS1 errors: missing periodical'' in wikipedia(zh) , which including 'df' only.\n",
    "\n",
    "這在爬蟲所有在「分類:含有未知參數的引用的頁面」裡因為「df」參數而報錯的條目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1755 articles in 9 pages.\n"
     ]
    }
   ],
   "source": [
    "next_page = []\n",
    "url = \"https://zh.wikipedia.org/wiki/Category:引文格式1错误：periodical系列参数与模板类型不匹配\"\n",
    "next_page.append(url)\n",
    "page_num = 1\n",
    "num = 0\n",
    "pre_url = 'https://zh.wikipedia.org'\n",
    "\n",
    "check_url = []\n",
    "\n",
    "for page in next_page:\n",
    "    res = req.get(page)\n",
    "    soup = bs(res.text, \"lxml\")\n",
    "    once = 0\n",
    "    for link in soup.select('div#mw-pages a'):\n",
    "        link_title = link.get_text()\n",
    "        #print(link_title)\n",
    "\n",
    "        link_url = 'https://zh.wikipedia.org' + link.get('href')\n",
    "        #print(link_url)\n",
    "\n",
    "        if(link_title == \"下一页\" and once == 0) :\n",
    "            page_num += 1\n",
    "            next_page.append(link_url)\n",
    "            once += 1\n",
    "            break\n",
    "\n",
    "\n",
    "    for link in soup.select('div.mw-category-group li'):\n",
    "        num += 1\n",
    "\n",
    "        temp_url = link.a.get('href')\n",
    "        if(temp_url is not None):\n",
    "            temp_url = pre_url + temp_url\n",
    "            #print('https://zh.wikipedia.org' + link.a.get('href'))\n",
    "            temp_title = link.get_text()\n",
    "            #print(link.get_text())\n",
    "\n",
    "            check_url.append(temp_url)\n",
    "\n",
    "print(\"There are \" + str(num) + \" articles in \" + str(page_num) + \" pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Crooked Rain, Crooked Rain - 维基百科，自由的百科全书\n",
      "https://zh.wikipedia.org/wiki/Crooked_Rain,_Crooked_Rain\n",
      "2. CSI犯罪現場：謀殺的三維 - 维基百科，自由的百科全书\n",
      "https://zh.wikipedia.org/wiki/CSI%E7%8A%AF%E7%BD%AA%E7%8F%BE%E5%A0%B4%EF%BC%9A%E8%AC%80%E6%AE%BA%E7%9A%84%E4%B8%89%E7%B6%AD\n",
      "3. Cö shu Nie - 维基百科，自由的百科全书\n",
      "https://zh.wikipedia.org/wiki/C%C3%B6_shu_Nie\n",
      "4. 蠕形蟎屬 - 维基百科，自由的百科全书\n",
      "https://zh.wikipedia.org/wiki/%E8%A0%95%E5%BD%A2%E8%9F%8E%E5%B1%AC\n",
      "5. Dive Bar Tour - 维基百科，自由的百科全书\n",
      "https://zh.wikipedia.org/wiki/Dive_Bar_Tour\n",
      "6. Facebook Portal - 维基百科，自由的百科全书\n",
      "https://zh.wikipedia.org/wiki/Facebook_Portal\n",
      "7. Facetmobile - 维基百科，自由的百科全书\n",
      "https://zh.wikipedia.org/wiki/Facetmobile\n",
      "8. FaZe Clan - 维基百科，自由的百科全书\n",
      "https://zh.wikipedia.org/wiki/FaZe_Clan\n",
      "9. Game Boy - 维基百科，自由的百科全书\n",
      "https://zh.wikipedia.org/wiki/Game_Boy\n",
      "10. GT赛车3 A-Spec - 维基百科，自由的百科全书\n",
      "https://zh.wikipedia.org/wiki/GT%E8%B5%9B%E8%BD%A63_A-Spec\n",
      "==================================================\n",
      "There are 10 articles with error because 'magazine' parameter.\n"
     ]
    }
   ],
   "source": [
    "# magazine\n",
    "cnt = 0\n",
    "for page in check_url:\n",
    "    res = req.get(page)\n",
    "    soup = bs(res.text, \"lxml\")\n",
    "\n",
    "    for link in soup.select('span.error.citation-comment'):\n",
    "        link_txt = link.get_text()\n",
    "        # ppring(link_txt)\n",
    "\n",
    "        if(link_txt.find(\"magazine\") != -1):\n",
    "            cnt += 1\n",
    "            print(str(cnt) + \". \" + soup.title.text)\n",
    "            print(page)\n",
    "            break\n",
    "    \n",
    "    if(cnt >= 10): break\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"There are \" + str(cnt) + \" articles with error because 'magazine' parameter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newspaper\n",
    "cnt = 0\n",
    "for page in check_url:\n",
    "    res = req.get(page)\n",
    "    soup = bs(res.text, \"lxml\")\n",
    "\n",
    "    for link in soup.select('span.error.citation-comment'):\n",
    "        link_txt = link.get_text()\n",
    "        # ppring(link_txt)\n",
    "\n",
    "        if(link_txt.find(\"newspaper\") != -1):\n",
    "            cnt += 1\n",
    "            print(str(cnt) + \". \" + soup.title.text)\n",
    "            print(page)\n",
    "            break\n",
    "    \n",
    "    if(cnt >= 10): break\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"There are \" + str(cnt) + \" articles with error because 'newspaper' parameter.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
